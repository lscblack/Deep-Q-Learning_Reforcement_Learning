[
  {
    "experiment": 1,
    "mean_reward": 18.333333333333332,
    "std_reward": 3.39934634239519,
    "model_path": "cyiza_results\\exp_1\\final_model.zip",
    "hyperparameters": {
      "lr": 0.0008,
      "gamma": 0.9,
      "batch_size": 64,
      "eps_start": 1.0,
      "eps_end": 0.02,
      "eps_decay": 50000.0,
      "policy": "CnnPolicy",
      "buffer_size": 20000,
      "description": "Aggressive learner"
    }
  },
  {
    "experiment": 2,
    "mean_reward": 1.6666666666666667,
    "std_reward": 0.9428090415820634,
    "model_path": "cyiza_results\\exp_2\\final_model.zip",
    "hyperparameters": {
      "lr": 5e-06,
      "gamma": 0.999,
      "batch_size": 32,
      "eps_start": 1.0,
      "eps_end": 0.05,
      "eps_decay": 800000.0,
      "policy": "CnnPolicy",
      "buffer_size": 20000,
      "description": "Conservative slow learner"
    }
  },
  {
    "experiment": 3,
    "mean_reward": 18.666666666666668,
    "std_reward": 2.6246692913372702,
    "model_path": "cyiza_results\\exp_3\\final_model.zip",
    "hyperparameters": {
      "lr": 0.0003,
      "gamma": 0.98,
      "batch_size": 64,
      "eps_start": 1.0,
      "eps_end": 0.2,
      "eps_decay": 100000.0,
      "policy": "CnnPolicy",
      "buffer_size": 20000,
      "description": "High exploration"
    }
  },
  {
    "experiment": 4,
    "mean_reward": 7.0,
    "std_reward": 0.0,
    "model_path": "cyiza_results\\exp_4\\final_model.zip",
    "hyperparameters": {
      "lr": 0.0001,
      "gamma": 0.99,
      "batch_size": 32,
      "eps_start": 1.0,
      "eps_end": 0.01,
      "eps_decay": 50000.0,
      "policy": "MlpPolicy",
      "buffer_size": 20000,
      "description": "Fast greedy collapse"
    }
  },
  {
    "experiment": 5,
    "mean_reward": 16.666666666666668,
    "std_reward": 4.988876515698588,
    "model_path": "cyiza_results\\exp_5\\final_model.zip",
    "hyperparameters": {
      "lr": 0.0002,
      "gamma": 0.995,
      "batch_size": 128,
      "eps_start": 1.0,
      "eps_end": 0.02,
      "eps_decay": 300000.0,
      "policy": "CnnPolicy",
      "buffer_size": 20000,
      "description": "Stable learner"
    }
  },
  {
    "experiment": 6,
    "mean_reward": 10.0,
    "std_reward": 2.943920288775949,
    "model_path": "cyiza_results\\exp_6\\final_model.zip",
    "hyperparameters": {
      "lr": 0.00015,
      "gamma": 0.97,
      "batch_size": 8,
      "eps_start": 1.0,
      "eps_end": 0.05,
      "eps_decay": 1000000.0,
      "policy": "CnnPolicy",
      "buffer_size": 20000,
      "description": "Noisy small batch"
    }
  },
  {
    "experiment": 7,
    "mean_reward": 21.333333333333332,
    "std_reward": 7.408703590297623,
    "model_path": "cyiza_results\\exp_7\\final_model.zip",
    "hyperparameters": {
      "lr": 0.0005,
      "gamma": 0.99,
      "batch_size": 64,
      "eps_start": 1.0,
      "eps_end": 0.01,
      "eps_decay": 2000000.0,
      "policy": "CnnPolicy",
      "buffer_size": 20000,
      "description": "Long exploration slow decay"
    }
  },
  {
    "experiment": 8,
    "mean_reward": 4.666666666666667,
    "std_reward": 2.0548046676563256,
    "model_path": "cyiza_results\\exp_8\\final_model.zip",
    "hyperparameters": {
      "lr": 0.0001,
      "gamma": 0.96,
      "batch_size": 64,
      "eps_start": 1.0,
      "eps_end": 0.02,
      "eps_decay": 80000.0,
      "policy": "MlpPolicy",
      "buffer_size": 20000,
      "description": "Short memory"
    }
  },
  {
    "experiment": 9,
    "mean_reward": 16.0,
    "std_reward": 2.943920288775949,
    "model_path": "cyiza_results\\exp_9\\final_model.zip",
    "hyperparameters": {
      "lr": 0.0003,
      "gamma": 0.999,
      "batch_size": 64,
      "eps_start": 1.0,
      "eps_end": 0.01,
      "eps_decay": 400000.0,
      "policy": "CnnPolicy",
      "buffer_size": 20000,
      "description": "Long horizon"
    }
  },
  {
    "experiment": 10,
    "mean_reward": 3.3333333333333335,
    "std_reward": 0.4714045207910317,
    "model_path": "cyiza_results\\exp_10\\final_model.zip",
    "hyperparameters": {
      "lr": 0.0002,
      "gamma": 0.92,
      "batch_size": 32,
      "eps_start": 0.8,
      "eps_end": 0.05,
      "eps_decay": 50000.0,
      "policy": "MlpPolicy",
      "buffer_size": 20000,
      "description": "Semi-greedy short-term"
    }
  }
]